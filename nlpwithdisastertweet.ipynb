{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\ntrain_df = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest_df = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\ntrain_df.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T12:53:59.640015Z","iopub.execute_input":"2025-06-06T12:53:59.640288Z","iopub.status.idle":"2025-06-06T12:54:02.239927Z","shell.execute_reply.started":"2025-06-06T12:53:59.640261Z","shell.execute_reply":"2025-06-06T12:54:02.239055Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Checking for missing values\ntrain_df.isnull().sum()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T12:55:17.109225Z","iopub.execute_input":"2025-06-06T12:55:17.109578Z","iopub.status.idle":"2025-06-06T12:55:17.118816Z","shell.execute_reply.started":"2025-06-06T12:55:17.109553Z","shell.execute_reply":"2025-06-06T12:55:17.117955Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Dropping rows with missing keywords or locations \ntrain_df = train_df.dropna(subset=['keyword', 'location'])\n\n# Reseting index after dropping\ntrain_df.reset_index(drop=True, inplace=True)\n\n# Checking new shape of the data\ntrain_df.shape\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T12:56:05.891010Z","iopub.execute_input":"2025-06-06T12:56:05.891320Z","iopub.status.idle":"2025-06-06T12:56:05.911496Z","shell.execute_reply.started":"2025-06-06T12:56:05.891297Z","shell.execute_reply":"2025-06-06T12:56:05.910415Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Plotting the target distribution\nsns.countplot(x='target', data=train_df)\nplt.title('Distribution of Disaster vs Non-Disaster Tweets')\nplt.xlabel('Target (0 = Non-disaster, 1 = Disaster)')\nplt.ylabel('Count')\nplt.show()\n\n# Displaying counts as text\nprint(train_df['target'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T13:05:00.914291Z","iopub.execute_input":"2025-06-06T13:05:00.914648Z","iopub.status.idle":"2025-06-06T13:05:02.280581Z","shell.execute_reply.started":"2025-06-06T13:05:00.914624Z","shell.execute_reply":"2025-06-06T13:05:02.279248Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# How many unique keywords?\nprint(\"Number of unique keywords:\", train_df['keyword'].nunique())\n\n# Most frequent keywords\ntop_keywords = train_df['keyword'].value_counts().head(15)\n\n# Plotting top keywords\nplt.figure(figsize=(10,6))\nsns.barplot(y=top_keywords.index, x=top_keywords.values)\nplt.title('Top 15 Most Frequent Keywords')\nplt.xlabel('Frequency')\nplt.ylabel('Keyword')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T13:05:57.622341Z","iopub.execute_input":"2025-06-06T13:05:57.623729Z","iopub.status.idle":"2025-06-06T13:05:57.899976Z","shell.execute_reply.started":"2025-06-06T13:05:57.623684Z","shell.execute_reply":"2025-06-06T13:05:57.899059Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Top 15 locations\ntop_locations = train_df['location'].value_counts().head(15)\n\nplt.figure(figsize=(10,6))\nsns.barplot(y=top_locations.index, x=top_locations.values)\nplt.title('Top 15 Tweet Locations')\nplt.xlabel('Frequency')\nplt.ylabel('Location')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T13:06:20.734559Z","iopub.execute_input":"2025-06-06T13:06:20.734933Z","iopub.status.idle":"2025-06-06T13:06:21.007792Z","shell.execute_reply.started":"2025-06-06T13:06:20.734905Z","shell.execute_reply":"2025-06-06T13:06:21.006732Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Creating new features\ntrain_df['text_length'] = train_df['text'].apply(len)\ntrain_df['word_count'] = train_df['text'].apply(lambda x: len(x.split()))\n\n# Plotting tweet length distribution by class\nplt.figure(figsize=(10,6))\nsns.histplot(data=train_df, x='text_length', hue='target', bins=40, kde=True)\nplt.title('Tweet Length Distribution by Target')\nplt.xlabel('Text Length (characters)')\nplt.ylabel('Count')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T13:06:52.822139Z","iopub.execute_input":"2025-06-06T13:06:52.822448Z","iopub.status.idle":"2025-06-06T13:06:53.286493Z","shell.execute_reply.started":"2025-06-06T13:06:52.822425Z","shell.execute_reply":"2025-06-06T13:06:53.285559Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\n\n# Downloading stopwords if not already done\nnltk.download('stopwords')\nstop_words = set(stopwords.words('english'))\nstemmer = PorterStemmer()\n\ndef clean_text(text):\n    text = text.lower()                                # lowercase\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # removing URLs\n    text = re.sub(r'\\@w+|\\#','', text)                 # removing mentions and hashtags\n    text = re.sub(r'\\d+', '', text)                    # removing numbers\n    text = text.translate(str.maketrans('', '', string.punctuation))  # removing punctuation\n    words = text.split()\n    words = [stemmer.stem(word) for word in words if word not in stop_words]\n    return \" \".join(words)\n\n# Applying cleaning\ntrain_df['clean_text'] = train_df['text'].apply(clean_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T13:08:25.910576Z","iopub.execute_input":"2025-06-06T13:08:25.911021Z","iopub.status.idle":"2025-06-06T13:08:28.004113Z","shell.execute_reply.started":"2025-06-06T13:08:25.910993Z","shell.execute_reply":"2025-06-06T13:08:28.003193Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df[['text', 'clean_text']].head(5)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T13:08:54.113978Z","iopub.execute_input":"2025-06-06T13:08:54.114303Z","iopub.status.idle":"2025-06-06T13:08:54.125710Z","shell.execute_reply.started":"2025-06-06T13:08:54.114281Z","shell.execute_reply":"2025-06-06T13:08:54.124822Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from wordcloud import WordCloud\n\n# Separating clean texts by class\ndisaster_tweets = train_df[train_df['target'] == 1]['clean_text']\nnon_disaster_tweets = train_df[train_df['target'] == 0]['clean_text']\n\n# Generating WordClouds\nplt.figure(figsize=(12,6))\nwordcloud1 = WordCloud(width=600, height=400, background_color='white').generate(\" \".join(disaster_tweets))\nplt.subplot(1, 2, 1)\nplt.imshow(wordcloud1, interpolation='bilinear')\nplt.axis('off')\nplt.title('Disaster Tweets WordCloud')\n\nwordcloud0 = WordCloud(width=600, height=400, background_color='white').generate(\" \".join(non_disaster_tweets))\nplt.subplot(1, 2, 2)\nplt.imshow(wordcloud0, interpolation='bilinear')\nplt.axis('off')\nplt.title('Non-Disaster Tweets WordCloud')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T13:10:15.378439Z","iopub.execute_input":"2025-06-06T13:10:15.378892Z","iopub.status.idle":"2025-06-06T13:10:17.569967Z","shell.execute_reply.started":"2025-06-06T13:10:15.378865Z","shell.execute_reply":"2025-06-06T13:10:17.568551Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# Initializing TF-IDF Vectorizer\ntfidf = TfidfVectorizer(max_features=5000)  # You can try 10,000 later\n\n# Fit on training clean text and transform\nX = tfidf.fit_transform(train_df['clean_text'])\n\n# Targeting labels\ny = train_df['target']\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T13:11:41.299363Z","iopub.execute_input":"2025-06-06T13:11:41.299704Z","iopub.status.idle":"2025-06-06T13:11:41.385414Z","shell.execute_reply.started":"2025-06-06T13:11:41.299680Z","shell.execute_reply":"2025-06-06T13:11:41.384338Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, accuracy_score\n\n# Splitting into train and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Training logistic regression\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\n# Predicting on validation set\ny_pred = model.predict(X_val)\n\n# Evaluating\nprint(\"Validation Accuracy:\", accuracy_score(y_val, y_pred))\nprint(\"\\nClassification Report:\\n\", classification_report(y_val, y_pred))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T13:12:35.788036Z","iopub.execute_input":"2025-06-06T13:12:35.789034Z","iopub.status.idle":"2025-06-06T13:12:36.079420Z","shell.execute_reply.started":"2025-06-06T13:12:35.788995Z","shell.execute_reply":"2025-06-06T13:12:36.078222Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cleaning test data using the same function\ntest_df['clean_text'] = test_df['text'].apply(clean_text)\n\n# Transforming test data using the same TF-IDF (never fit again!)\nX_test = tfidf.transform(test_df['clean_text'])\n\n# Making predictions\ntest_preds = model.predict(X_test)\n\n# Preparing submission DataFrame\nsubmission = pd.DataFrame({\n    'id': test_df['id'],\n    'target': test_preds\n})\n\n# Saving to CSV\nsubmission.to_csv('submission.csv', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T13:18:56.105857Z","iopub.execute_input":"2025-06-06T13:18:56.106216Z","iopub.status.idle":"2025-06-06T13:18:56.715195Z","shell.execute_reply.started":"2025-06-06T13:18:56.106192Z","shell.execute_reply":"2025-06-06T13:18:56.714026Z"}},"outputs":[],"execution_count":null}]}